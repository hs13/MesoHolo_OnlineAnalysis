{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9bda26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'suite2p.run_s2p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-524aa8270feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuite2p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msuite2p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_s2p\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_s2p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mScanImageTiffReader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScanImageTiffReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'suite2p.run_s2p'"
     ]
    }
   ],
   "source": [
    "# use when recorded both PMT channels, and want to suite2p only the green channel\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import suite2p\n",
    "from suite2p.run_s2p import run_s2p\n",
    "from ScanImageTiffReader import ScanImageTiffReader\n",
    "import multiprocessing\n",
    "from scipy.io import savemat\n",
    "import sys\n",
    "import logging\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf7987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save just the green channel as h5 file\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import suite2p\n",
    "from suite2p.run_s2p import run_s2p\n",
    "from ScanImageTiffReader import ScanImageTiffReader\n",
    "import multiprocessing\n",
    "from scipy.io import savemat\n",
    "import sys\n",
    "import logging\n",
    "from glob import glob\n",
    "\n",
    "def tiffs2array_meso(movie_list, x_slice, y_slice, t_slice,planes,channels, opsjson):\n",
    "    data = [LoadTif(str(mov),planes,channels) for mov in movie_list] #CAREFUL HERE should slice after concatenation for Satsuma RF, but NOT for Frankenrig\n",
    "    numtimepointstiffile = [d.shape[0] for d in data]\n",
    "#     print(numtimepointstiffile)\n",
    "    data = np.concatenate(data)\n",
    "    print('concatenated data from all tiffs, total size: ' + str(data.shape))\n",
    "    \n",
    "    print('number of frames: ' + str(data.shape[0]))\n",
    "    print('opsjson[''lines''] shape: ' + str( np.shape(opsjson['lines']) ))\n",
    "    print('opsjson[''dx'']: ' + str(opsjson['dx']))\n",
    "    print(data.shape[0], np.shape(opsjson['lines'])[1], np.shape(opsjson['lines'])[0]*opsjson['dx'][1] )\n",
    "\n",
    "    ylen = np.shape(opsjson['lines'])[1]\n",
    "    xlen = np.shape(opsjson['lines'])[0]*opsjson['dx'][1]\n",
    "    xbounds = opsjson['dx'].copy()\n",
    "    xbounds.append(xlen)\n",
    "    datars = np.zeros((data.shape[0], ylen, xlen ), dtype=np.int16)\n",
    "    for istrip in range(np.shape(opsjson['lines'])[0]):\n",
    "        x_strip= slice(xbounds[istrip],xbounds[istrip+1])\n",
    "        y_strip= slice(0,ylen)\n",
    "        datars[t_slice,y_strip,x_strip] = data[:,opsjson['lines'][istrip],:].copy()\n",
    "\n",
    "    print('reshaped data size: ' + str(datars.shape))\n",
    "    data = datars[t_slice, y_slice, x_slice] #.copy()\n",
    "    return data, numtimepointstiffile\n",
    "\n",
    "def LoadTif(mov_path,planes,channels):\n",
    "    with ScanImageTiffReader(mov_path) as reader:\n",
    "        data = reader.data()\n",
    "    Tend = int( np.floor(data.shape[0]/(planes*channels)) * (planes*channels) )\n",
    "    t_slice = slice(0,Tend, channels)\n",
    "    data = data[t_slice,:,:]\n",
    "    return data\n",
    "\n",
    "def crop_and_save_h5_meso(movs,tiff_folder,out_path,expno, x_slice,y_slice,planes,channels, opsjson):\n",
    "    # takes a list of tifs. Concatenates each plane, then slices and crops the big movie, then saves it as .h5 file in a folder inside tiff_folder\\\n",
    "    # process multiple folders\n",
    "    # instead of making one h5 per plane, just make it into one big h5\n",
    "    # movs: list of tifs to be processed\n",
    "    # tif folder: directory where the result h5 file will be saved (inside the corresponding plane folder)\n",
    "    # x_slice: cropping of the FOV in x\n",
    "    # x_slice: cropping of the FOV in y\n",
    "    # planes: number of planes for each tif file\n",
    "    # channels: number of channels\"\"\"\n",
    "    tic = time.time()\n",
    "\n",
    "    #exp_name1 = exp_name.replace(\"//\",\"\")\n",
    "    outname = [0]*planes\n",
    "    outdir=[0]*planes\n",
    "    try:\n",
    "        os.mkdir(out_path)\n",
    "    except:\n",
    "        if os.path.isdir(out_path):\n",
    "            print('directory already exists')\n",
    "        else:\n",
    "            print('Cannot create directory.. probably the name is wrong')\n",
    "            print(out_path)\n",
    "            \n",
    "\n",
    "    t_slice = slice(0, None, 1)\n",
    "\n",
    "    cropped_mov, numtimepointstiffile = tiffs2array_meso(movs,x_slice,y_slice,t_slice,planes,channels, opsjson)\n",
    "    print(cropped_mov.shape)\n",
    "    \n",
    "    plane = 0\n",
    "#     #for plane in np.arange(0,planes):\n",
    "#     t_slice = slice(plane*channels, cropped_mov.shape[0], channels * planes) \n",
    "    t_slice = slice(plane,cropped_mov.shape[0], planes)\n",
    "    x_slice = slice(0,cropped_mov.shape[2], 1)\n",
    "    y_slice = slice(0,cropped_mov.shape[1], 1)\n",
    "#     t_slice = slice(0,None, 1)\n",
    "#     x_slice = slice(0,None, 1) \n",
    "#     y_slice = slice(0,None, 1)\n",
    "    outdir[plane] = out_path #+'plane_'+ str(plane) + '//'\n",
    "    try:\n",
    "        os.mkdir(outdir[plane])\n",
    "    except:\n",
    "        print('couldnt make directory')\n",
    "        print(str(outdir[plane]))\n",
    "\n",
    "    outname[plane]=outdir[plane] + 'cropped_mov_' + str(expno) + '.h5'\n",
    "    print(f'now processing {outname[plane]}')\n",
    "    # try:\n",
    "    hf = h5py.File(outname[plane], 'a')\n",
    "    cropped_mov1 = cropped_mov[t_slice, y_slice, x_slice]\n",
    "    print(cropped_mov1.shape)\n",
    "    hf.create_dataset('data', data=cropped_mov1, dtype= 'uint16')\n",
    "    hf.close()\n",
    "    print('done saving ' + str(outname[plane]))\n",
    "    # delete the array and clean RAM\n",
    "    del cropped_mov\n",
    "    del cropped_mov1\n",
    "    gc.collect()\n",
    "    # except:\n",
    "    #     print(f'the file {outname[plane]} already exists')\n",
    "\n",
    "    toc = time.time() - tic\n",
    "    print('All saved,took  ' + str(toc) + 'secs')\n",
    "\n",
    "    return numtimepointstiffile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df1d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER NEEDS TO PUT INFO EVERY EXPT\n",
    "tic1 = time.time()\n",
    "\n",
    "# INFO ABOUT YOUR FILES\n",
    "baseFreq = 4.55\n",
    "channels = 2\n",
    "planes = 1\n",
    "\n",
    "# run the following lines in matlab\n",
    "# xyorig = [50 50; fullnpix_orig(1)-50 fullnpix_orig(2)-50];\n",
    "# xynew = convertcoords_HoloFOVtoCurrentFOV(hSI,xyorig, fullnpix_orig, fullxsize_orig, fullysize_orig, fullxcenter_orig, fullycenter_orig)\n",
    "\n",
    "# # 221220: HS_Ai203_2\n",
    "# # xynew =\n",
    "# #          404         217\n",
    "# #         1508         767\n",
    "# x_start = 404; # horizontal. xynew(1,1)\n",
    "# x_end = 1508; # horizontal. xynew(2,1)\n",
    "# y_start = 217; # xynew(1,2)\n",
    "# y_end = 767; # xynew(2,2)\n",
    "\n",
    "# # 221221: 3x3 shifted bottom left (holoFOV is in the posterior center of FOV)\n",
    "# # xynew =\n",
    "# #          968         217\n",
    "# #         2072         767\n",
    "# x_start = 968; # horizontal. xynew(1,1)\n",
    "# x_end = 2072; # horizontal. xynew(2,1)\n",
    "# y_start = 217; # xynew(1,2)\n",
    "# y_end = 767; # xynew(2,2)\n",
    "\n",
    "# # 221226: HS_Ai203_16\n",
    "# # xynew =\n",
    "# #          350         125\n",
    "# #         1450         675\n",
    "# x_start = 350; # horizontal. xynew(1,1)\n",
    "# x_end = 1450; # horizontal. xynew(2,1)\n",
    "# y_start = 125; # xynew(1,2)\n",
    "# y_end = 675; # xynew(2,2)\n",
    "\n",
    "# # lateral 2pt4x2pt4 4pt55hz\n",
    "# x_start = 1; # horizontal. xynew(1,1)\n",
    "# x_end = 1101; # horizontal. xynew(2,1)\n",
    "# y_start = 125; # xynew(1,2)\n",
    "# y_end = 675; # xynew(2,2)\n",
    "\n",
    "# # HS_Ai203_16_4pt55hz_shiftright600\n",
    "# x_start = 1 #-250? # horizontal. xynew(1,1)\n",
    "# x_end = 851 # horizontal. xynew(2,1)\n",
    "# y_start = 125 # xynew(1,2)\n",
    "# y_end = 675 # xynew(2,2)\n",
    "\n",
    "# HS_Ai203_16_4pt55hz_shiftdown100\n",
    "#          350          75\n",
    "#         1450         625\n",
    "x_start = 350; # horizontal. xynew(1,1)\n",
    "x_end = 1450; # horizontal. xynew(2,1)\n",
    "y_start = 75; # xynew(1,2)\n",
    "y_end = 625; # xynew(2,2)\n",
    "\n",
    "# drive = 'M://'\n",
    "# user_name = 'ICmesoholoexpts_scanimage'\n",
    "drive = 'D://'\n",
    "user_name = 'HS'\n",
    "mouse = 'MU31_1'\n",
    "date = '230111'\n",
    "expList = ['staticICtxi0', 'staticgratings', 'staticgratings12']\n",
    "\n",
    "#where your tifs are\n",
    "tiffFolderList = [drive+ user_name + '//' + mouse +'//' + date + '//' + exp_name + '//' for exp_name in expList]\n",
    "#where you want to save the output hdf5 file\n",
    "suffix = 'ClosedLoop_justgreen//' #'suite2pAnalysis//'\n",
    "data_path = drive+ user_name + '//' + mouse +'//' + date + '//'\n",
    "out_path = drive+ user_name + '//' + mouse +'//' + date + '//' + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfeb160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKE SURE TO RUN mesoscope_json_from_scanimage_210617_59.m IN MATLAB 2021b !!!\n"
     ]
    }
   ],
   "source": [
    "print('MAKE SURE TO RUN mesoscope_json_from_scanimage_210617_59.m IN MATLAB 2021b !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d425f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if x_start>=1:\n",
    "#     x_slice= slice(x_start-1,x_end)\n",
    "# else:\n",
    "#     x_slice= slice(0,x_end)\n",
    "# if y_start>=1:\n",
    "#     y_slice= slice(y_start-1,y_end)\n",
    "# else:\n",
    "#     y_slice= slice(0,y_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb208b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fs', 'nplanes', 'nrois', 'mesoscan', 'diameter', 'num_workers_roi', 'keep_movie_raw', 'delete_bin', 'batch_size', 'nimg_init', 'tau', 'combined', 'nonrigid', 'dx', 'dy', 'lines', 'data_path', 'save_path0'])\n",
      "concatenated data from all tiffs, total size: (6766, 5166, 600)\n",
      "number of frames: 6766\n",
      "opsjson[lines] shape: (4, 1200)\n",
      "opsjson[dx]: [0, 600, 1200, 1800]\n",
      "6766 1200 2400\n",
      "reshaped data size: (6766, 1200, 2400)\n",
      "(6766, 551, 1101)\n",
      "couldnt make directory\n",
      "D://HS//MU31_1//230111//ClosedLoop_justgreen//\n",
      "now processing D://HS//MU31_1//230111//ClosedLoop_justgreen//cropped_mov_staticICtxi0.h5\n",
      "(6766, 551, 1101)\n",
      "done saving D://HS//MU31_1//230111//ClosedLoop_justgreen//cropped_mov_staticICtxi0.h5\n",
      "All saved,took  181.18992161750793secs\n",
      "directory already exists\n",
      "concatenated data from all tiffs, total size: (1478, 5166, 600)\n",
      "number of frames: 1478\n",
      "opsjson[lines] shape: (4, 1200)\n",
      "opsjson[dx]: [0, 600, 1200, 1800]\n",
      "1478 1200 2400\n",
      "reshaped data size: (1478, 1200, 2400)\n",
      "(1478, 551, 1101)\n",
      "couldnt make directory\n",
      "D://HS//MU31_1//230111//ClosedLoop_justgreen//\n",
      "now processing D://HS//MU31_1//230111//ClosedLoop_justgreen//cropped_mov_staticgratings.h5\n",
      "(1478, 551, 1101)\n",
      "done saving D://HS//MU31_1//230111//ClosedLoop_justgreen//cropped_mov_staticgratings.h5\n",
      "All saved,took  31.027992963790894secs\n",
      "directory already exists\n",
      "concatenated data from all tiffs, total size: (919, 5166, 600)\n",
      "number of frames: 919\n",
      "opsjson[lines] shape: (4, 1200)\n",
      "opsjson[dx]: [0, 600, 1200, 1800]\n",
      "919 1200 2400\n",
      "reshaped data size: (919, 1200, 2400)\n",
      "(919, 551, 1101)\n",
      "couldnt make directory\n",
      "D://HS//MU31_1//230111//ClosedLoop_justgreen//\n",
      "now processing D://HS//MU31_1//230111//ClosedLoop_justgreen//cropped_mov_staticgratings12.h5\n",
      "(919, 551, 1101)\n",
      "done saving D://HS//MU31_1//230111//ClosedLoop_justgreen//cropped_mov_staticgratings12.h5\n",
      "All saved,took  18.65066385269165secs\n",
      "231.2625699043274\n",
      "completed h5 conversion\n"
     ]
    }
   ],
   "source": [
    "# only took 130 sec    \n",
    "x_slice= slice(x_start-1,x_end)\n",
    "y_slice= slice(y_start-1,y_end)\n",
    "    \n",
    "tic1 = time.time()\n",
    "\n",
    "os.chdir(data_path+expList[0]+'//')\n",
    "with open('ops.json') as f:\n",
    "    opsjson = json.load(f)\n",
    "print(opsjson.keys())\n",
    "\n",
    "gc.collect()\n",
    "numtimepointstiffileagg = []\n",
    "for exp_name, tiff_folder in zip(expList,tiffFolderList):\n",
    "    gc.collect()\n",
    "    pth = Path(tiff_folder)\n",
    "    movs = list(pth.glob('*.tif'))\n",
    "    numtimepointstiffile = crop_and_save_h5_meso(movs,tiff_folder,out_path,exp_name, x_slice,y_slice,planes,channels, opsjson)\n",
    "    numtimepointstiffileagg.append(numtimepointstiffile.copy())\n",
    "toc1 = time.time()-tic1\n",
    "print(toc1)\n",
    "print('completed h5 conversion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ade2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MesoDAQ\\anaconda3\\envs\\CLbehavior\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "savemat(out_path+'online_params.mat', {'expList':expList, 'numchannels':channels, 'Nplanes':planes, 'x_start':x_start, 'x_end':x_end, 'y_start':y_start, 'y_end':y_end, 'numtimepointstiffile':numtimepointstiffileagg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a09c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # troubleshooting rewriting tiffs2array_meso\n",
    "# os.chdir(data_path)\n",
    "# with open('ops.json') as f:\n",
    "#     opsjson = json.load(f)\n",
    "# print(opsjson.keys())\n",
    "\n",
    "# tiff_folder = 'D://HS//HS_Ai203_2//220531//retinotopy2//'\n",
    "# pth = Path(tiff_folder)\n",
    "# movie_list = list(pth.glob('*.tif'))\n",
    "# # movs[:10]\n",
    "# data = [LoadTif(str(mov),planes,channels) for mov in movie_list] #CAREFUL HERE should slice after concatenation for Satsuma RF, but NOT for Frankenrig\n",
    "# data = np.concatenate(data)\n",
    "# print('concatenated data from all tiffs, total size: ' + str(data.shape))\n",
    "# print('number of frames: ' + str(data.shape[0]))\n",
    "# print('opsjson[''lines''] shape: ' + str( np.shape(opsjson['lines']) ))\n",
    "# print('opsjson[''dx'']: ' + str(opsjson['dx']))\n",
    "# print(data.shape[0], np.shape(opsjson['lines'])[1], np.shape(opsjson['lines'])[0]*opsjson['dx'][1] )\n",
    "\n",
    "# # # Matlab code for reshaping the data\n",
    "# # # in Matlab, tif data size 608 7620 numframes, and reshaped data size is 1484 3040 numframes\n",
    "# # % reshape the data\n",
    "# # tempdata = zeros(max(Ly), sum(Lx), size(temp,3), 'int16');\n",
    "# # for istrip = 1:numel(Ly)\n",
    "# #     tempdata(1:Ly(istrip), sum(Lx(1:istrip-1))+1:sum(Lx(1:istrip)), :) = permute(temp(:,data.lines{istrip}+1,:),[2 1 3]);\n",
    "# # end\n",
    "# # % crop the data\n",
    "# # temp = tempdata(ylb:yub, xlb:xub, :);\n",
    "# x_slice= slice(x_start,x_end+1)\n",
    "# y_slice= slice(y_start,y_end+1)\n",
    "\n",
    "# tic1 = time.time()\n",
    "# ylen = np.shape(opsjson['lines'])[1]\n",
    "# xlen = np.shape(opsjson['lines'])[0]*opsjson['dx'][1]\n",
    "# xbounds = opsjson['dx'].copy()\n",
    "# xbounds.append(xlen)\n",
    "# datars = np.zeros((data.shape[0], ylen, xlen ))\n",
    "# for istrip in range(np.shape(opsjson['lines'])[0]):\n",
    "#     x_strip= slice(xbounds[istrip],xbounds[istrip+1])\n",
    "#     y_strip= slice(0,ylen)\n",
    "#     print(data[:,opsjson['lines'][istrip],:].shape)\n",
    "#     datars[:,y_strip,x_strip] = data[:,opsjson['lines'][istrip],:].copy()\n",
    "\n",
    "# data = datars[:, y_slice, x_slice].copy()\n",
    "# print('reshaped data size: ' + str(datars.shape))\n",
    "# print('cropped data size: ' + str(data.shape))\n",
    "\n",
    "# toc1 = time.time()-tic1\n",
    "# print(toc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a7365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or if you want to play with the parameters define everything here:\n",
    "# Should comment what everything is, not sure about all the parameters.\n",
    "ops = {'look_one_level_down': 0.0,\n",
    " 'fast_disk': [],\n",
    " 'delete_bin': True,\n",
    " 'mesoscan': False,\n",
    " 'bruker': False,\n",
    " 'h5py': [],\n",
    " 'h5py_key': 'data',\n",
    " 'save_path0': [],\n",
    " 'save_folder': [],\n",
    " 'subfolders': [],\n",
    " 'move_bin': False,\n",
    " 'nplanes': 1,\n",
    " 'nchannels': 1,\n",
    " 'functional_chan': 1,\n",
    " 'tau': 1.5,\n",
    " 'fs': baseFreq/planes,\n",
    " 'force_sktiff': False,\n",
    " 'frames_include': -1,\n",
    " 'multiplane_parallel': False,\n",
    " 'preclassify': 0.0,\n",
    " 'save_mat': True,\n",
    " 'save_NWB': False,\n",
    " 'combined': 1.0,\n",
    " 'aspect': 2.0,\n",
    " 'do_bidiphase': True,\n",
    " 'bidiphase': 0,\n",
    "#  'bidi_corrected': False,\n",
    " 'do_registration': 1,\n",
    " 'two_step_registration': False,\n",
    " 'keep_movie_raw': False, # if two_step_registration = 1 this has to also be true.\n",
    " 'nimg_init': 1000,\n",
    " 'batch_size': 2000,\n",
    " 'maxregshift': 0.1,\n",
    " 'align_by_chan': 1,\n",
    " 'reg_tif': False,\n",
    " 'reg_tif_chan2': False,\n",
    " 'subpixel': 10,\n",
    " 'smooth_sigma_time': 1.0, # default 0\n",
    " 'smooth_sigma': 1.15, # pixels, default 1.15\n",
    " 'th_badframes': 1.0,\n",
    " 'pad_fft': False,\n",
    " 'nonrigid': False,\n",
    " 'block_size': [128, 128],\n",
    " 'snr_thresh': 1.2,\n",
    " 'maxregshiftNR': 5.0,\n",
    " '1Preg': False,\n",
    " 'spatial_hp': 25,\n",
    " 'spatial_hp_reg': 26.0,\n",
    " 'spatial_hp_detect': 25,\n",
    " 'pre_smooth': 0,\n",
    " 'spatial_taper': 50.0,\n",
    " 'roidetect': True,\n",
    " 'spikedetect': True,\n",
    " 'sparse_mode': False,\n",
    " 'diameter': 9,\n",
    " 'spatial_scale': 1, # default 0, if set to 0, then the algorithm determines it automatically (recommend this on the first try)\n",
    " 'connected': True,\n",
    " 'nbinned': 5000, # maximum number of binned frames to use for ROI detection\n",
    " 'max_iterations': 100, # default 20. how many iterations over which to extract cells - at most ops[‘max_iterations’], \n",
    "                        #but usually stops before due to ops[‘threshold_scaling’] criterion\n",
    " 'threshold_scaling': 1.0, # default: 5.0. this controls the threshold at which to detect ROIs.. if you set this higher, then fewer ROIs will be detected\n",
    " 'max_overlap': 0.5, # default: 0.75\n",
    " 'high_pass': 100, #  default: 100. time window size\n",
    " 'inner_neuropil_radius': 2, # default: 2. number of pixels to keep between ROI and neuropil donut\n",
    " 'min_neuropil_pixels': 350, #default: 350. minimum number of pixels used to compute neuropil for each cell\n",
    " 'allow_overlap': False, #default: False. whether or not to extract signals from pixels which belong to two ROIs.\n",
    " 'chan2_thres': 0.65,\n",
    " 'baseline': 'maximin',\n",
    " 'win_baseline': 60.0,\n",
    " 'sig_baseline': 10.0,\n",
    " 'prctile_baseline': 8.0,\n",
    " 'neucoeff': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9947e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "h5\n",
      "** Found 3 h5 files - converting to binary **\n",
      "NOTE: using a list of h5 files:\n",
      "['D://HS//MU31_1//230111//ClosedLoop_justgreen\\\\cropped_mov_staticICtxi0.h5', 'D://HS//MU31_1//230111//ClosedLoop_justgreen\\\\cropped_mov_staticgratings12.h5', 'D://HS//MU31_1//230111//ClosedLoop_justgreen\\\\cropped_mov_staticgratings.h5']\n",
      "time 71.58 sec. Wrote 9163 frames per binary for 1 planes\n",
      ">>>>>>>>>>>>>>>>>>>>> PLANE 0 <<<<<<<<<<<<<<<<<<<<<<\n",
      "NOTE: not registered / registration forced with ops['do_registration']>1\n",
      "      (no previous offsets to delete)\n",
      "----------- REGISTRATION\n",
      "registering 9163 frames\n",
      "NOTE: estimated bidiphase offset from data: 0 pixels\n",
      "Reference frame, 268.59 sec.\n",
      "Registered 8000/9163 in 466.92s\n",
      "added enhanced mean image\n",
      "----------- Total 870.40 sec\n",
      "Registration metrics, 50.00 sec.\n",
      "NOTE: applying default C:\\Users\\MesoDAQ\\.suite2p\\classifiers\\classifier_user.npy\n",
      "----------- ROI DETECTION\n",
      "Binning movie in chunks of length 07\n",
      "Binned movie [1278,543,1085] in 34.10 sec.\n",
      "ROIs: 200, cost: 0.5812, time: 56.6441\n",
      "ROIs: 400, cost: 0.5625, time: 80.3381\n",
      "ROIs: 600, cost: 0.5506, time: 104.4229\n",
      "ROIs: 800, cost: 0.5423, time: 129.1221\n",
      "ROIs: 1000, cost: 0.5360, time: 153.9097\n",
      "ROIs: 1200, cost: 0.5308, time: 179.9386\n",
      "ROIs: 1383, cost: 0.5266, time: 206.4726\n",
      "ROIs: 1412, cost: 0.5257, time: 228.5256\n",
      "ROIs: 1416, cost: 0.5254, time: 250.3538\n",
      "ROIs: 1416, cost: 0.5743, time: 265.5398\n",
      "ROIs: 1416, cost: 0.5593, time: 281.9185\n",
      "ROIs: 1416, cost: 0.5549, time: 297.4792\n",
      "Detected 1416 ROIs, 322.76 sec\n",
      "After removing overlaps, 1219 ROIs remain\n",
      "----------- Total 360.76 sec.\n",
      "----------- EXTRACTION\n",
      "Masks created, 5.72 sec.\n",
      "Extracted fluorescence from 1219 ROIs in 9163 frames, 27.58 sec.\n",
      "----------- Total 33.91 sec.\n",
      "----------- CLASSIFICATION\n",
      "['compact', 'npix_norm', 'skew']\n",
      "----------- Total 0.03 sec.\n",
      "----------- SPIKE DECONVOLUTION\n",
      "----------- Total 0.67 sec.\n",
      "deleting binary files\n",
      "Plane 0 processed in 1319.23 sec (can open in GUI).\n",
      "total = 1391.02 sec.\n",
      "TOTAL RUNTIME 1391.02 sec\n",
      "Suite2p took 23.183718895912172mins\n"
     ]
    }
   ],
   "source": [
    "# when running only one plane, takes the same amount of time (12 min in benchmark data)\n",
    "tic2 = time.time()\n",
    "ops['data_path'] = [out_path]\n",
    "ops['h5py'] = out_path\n",
    "ops1 = run_s2p(ops = ops, db = {})\n",
    "toc2 = time.time() - tic2\n",
    "print('Suite2p took ' + str(toc2/60) + 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26c8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # suite2p took 681s, or 12 min\n",
    "# #list of files to analyze:\n",
    "# outname = [0]*planes\n",
    "# outdir=[0]*planes \n",
    "\n",
    "# #save output data\n",
    "# for i in range(planes):\n",
    "#     outdir[i]= out_path + 'plane_'+str(i)+'//'\n",
    "#     outname[i]= out_path + 'plane_'+str(i)+'//'\n",
    "\n",
    "# print(outdir)\n",
    "# print(outname[0])\n",
    "\n",
    "# # prepare the processes for each plane\n",
    "# db = []\n",
    "# jobs = []\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     for i in range(planes):\n",
    "#     #while the # of planes is less than the max number of cores we want to use (leaving 2 cores for scanimage)\n",
    "#        if i<os.cpu_count()-2:\n",
    "#     # Define the dataset\n",
    "#             this_db = {\n",
    "#               'h5py':outname[i], # a single h5 file path[p]\n",
    "#               'h5py_key': ['data'],\n",
    "#               'look_one_level_down': True, # whether to look in ALL subfolders when searching for tiffs,\n",
    "#               'save_path0': outdir[i],\n",
    "#               'data_path':  [], # a list of folders with tiffs\n",
    "#                                                      # (or folder of folders with tiffs if look_one_level_down is True, or subfolders is not empty)\n",
    "#               'subfolders': [], # choose subfolders of 'data_path' to look in (optional)\n",
    "#               'fast_disk': [] }# string which specifies where the binary file will be stored (should be an SSD)      \n",
    "#             print(f'Will be processing this data : {this_db}')\n",
    "#             db.append(this_db)\n",
    "#             p = multiprocessing.Process(target=run_s2p, args=(ops,this_db))\n",
    "#             p.start()          \n",
    "#             jobs.append(p)\n",
    "#        else:\n",
    "#             print(f'Hey, do you really want to run all these {i} cores at the same time?')\n",
    "            \n",
    "# #run each plane in parallel\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logging.info('Starting suite2p parallel processing in a different CPU core for each plane')\n",
    "\n",
    "# tic = time.time()\n",
    "# if len(jobs)<os.cpu_count()-2:\n",
    "#     for job in jobs:\n",
    "#         #print ('Parallel processing:  ' + str(outname[i]))\n",
    "#         job.join()\n",
    "#     toc = time.time() - tic\n",
    "#     print('All saved,took  ' + str(toc) + 'secs')\n",
    "# else:\n",
    "#    print(f'Hey, do you really want to run all these {i} cores at the same time?')\n",
    "\n",
    "# toc2 = time.time() - tic1\n",
    "# print('Preprocessing took ' + str(toc2/60) + 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c48b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
