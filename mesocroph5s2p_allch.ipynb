{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use when recorded both PMT channels, and want to suite2p both channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all channels go as input to suite2p\n",
    "import os\n",
    "import suite2p\n",
    "from suite2p.run_s2p import run_s2p\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from ScanImageTiffReader import ScanImageTiffReader\n",
    "import h5py\n",
    "from scipy.io import savemat\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "def tiffs2array_meso(movie_list, x_slice, y_slice, t_slice,planes,channels, opsjson):\n",
    "    data = [LoadTif(str(mov),planes,channels) for mov in movie_list] #CAREFUL HERE should slice after concatenation for Satsuma RF, but NOT for Frankenrig\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    print('concatenated data from all tiffs, total size: ' + str(data.shape))\n",
    "    \n",
    "    print('number of frames: ' + str(data.shape[0]))\n",
    "    print('opsjson[''lines''] shape: ' + str( np.shape(opsjson['lines']) ))\n",
    "    print('opsjson[''dx'']: ' + str(opsjson['dx']))\n",
    "    print(data.shape[0], np.shape(opsjson['lines'])[1], np.shape(opsjson['lines'])[0]*opsjson['dx'][1] )\n",
    "\n",
    "    ylen = np.shape(opsjson['lines'])[1]\n",
    "    xlen = np.shape(opsjson['lines'])[0]*opsjson['dx'][1]\n",
    "    xbounds = opsjson['dx'].copy()\n",
    "    xbounds.append(xlen)\n",
    "    datars = np.zeros((data.shape[0], ylen, xlen ), dtype=np.int16)\n",
    "    for istrip in range(np.shape(opsjson['lines'])[0]):\n",
    "        x_strip= slice(xbounds[istrip],xbounds[istrip+1])\n",
    "        y_strip= slice(0,ylen)\n",
    "        datars[t_slice,y_strip,x_strip] = data[:,opsjson['lines'][istrip],:].copy()\n",
    "\n",
    "    print('reshaped data size: ' + str(datars.shape))\n",
    "    data = datars[t_slice, y_slice, x_slice] #.copy()\n",
    "    return data\n",
    "\n",
    "def LoadTif(mov_path,planes,channels):\n",
    "    with ScanImageTiffReader(mov_path) as reader:\n",
    "        data = reader.data()\n",
    "    Tend = int( np.floor(data.shape[0]/(planes*channels)) * (planes*channels) )\n",
    "    t_slice = slice(0,Tend, 1)\n",
    "    data = data[t_slice,:,:]\n",
    "    return data\n",
    "\n",
    "def crop_and_save_h5_meso(movs,tiff_folder,out_path,expno, x_slice,y_slice,planes,channels, opsjson):\n",
    "    # takes a list of tifs. Concatenates each plane, then slices and crops the big movie, then saves it as .h5 file in a folder inside tiff_folder\\\n",
    "    # process multiple folders\n",
    "    # instead of making one h5 per plane, just make it into one big h5\n",
    "    # movs: list of tifs to be processed\n",
    "    # tif folder: directory where the result h5 file will be saved (inside the corresponding plane folder)\n",
    "    # x_slice: cropping of the FOV in x\n",
    "    # x_slice: cropping of the FOV in y\n",
    "    # planes: number of planes for each tif file\n",
    "    # channels: number of channels\"\"\"\n",
    "    tic = time.time()\n",
    "\n",
    "    #exp_name1 = exp_name.replace(\"//\",\"\")\n",
    "    outname = [0]*planes\n",
    "    outdir=[0]*planes\n",
    "    try:\n",
    "        os.mkdir(out_path)\n",
    "    except:\n",
    "        if os.path.isdir(out_path):\n",
    "            print('directory already exists')\n",
    "        else:\n",
    "            print('Cannot create directory.. probably the name is wrong')\n",
    "            print(out_path)\n",
    "            \n",
    "\n",
    "    t_slice = slice(0, None, 1)\n",
    "\n",
    "    cropped_mov = tiffs2array_meso(movs,x_slice,y_slice,t_slice,planes,channels, opsjson)\n",
    "    print(cropped_mov.shape)\n",
    "    \n",
    "    plane = 0\n",
    "#     #for plane in np.arange(0,planes):\n",
    "#     t_slice = slice(plane*channels, cropped_mov.shape[0], channels * planes) \n",
    "#     x_slice = slice(0,cropped_mov.shape[2], 1)\n",
    "#     y_slice = slice(0,cropped_mov.shape[1], 1)\n",
    "    t_slice = slice(0,None, 1)\n",
    "    x_slice = slice(0,None, 1) \n",
    "    y_slice = slice(0,None, 1)\n",
    "    outdir[plane] = out_path #+'plane_'+ str(plane) + '//'\n",
    "    try:\n",
    "        os.mkdir(outdir[plane])\n",
    "    except:\n",
    "        print('couldnt make directory')\n",
    "        print(str(outdir[plane]))\n",
    "\n",
    "    outname[plane]=outdir[plane] + 'cropped_mov_' + str(expno) + '.h5'\n",
    "    print(f'now processing {outname[plane]}')\n",
    "    # try:\n",
    "    hf = h5py.File(outname[plane], 'a')\n",
    "    cropped_mov1 = cropped_mov[t_slice, y_slice, x_slice]\n",
    "    print(cropped_mov1.shape)\n",
    "    hf.create_dataset('data', data=cropped_mov1, dtype= 'uint16')\n",
    "    hf.close()\n",
    "    print('done saving ' + str(outname[plane]))\n",
    "    # delete the array and clean RAM\n",
    "    del cropped_mov1\n",
    "    del cropped_mov\n",
    "    gc.collect()\n",
    "    # except:\n",
    "    #     print(f'the file {outname[plane]} already exists')\n",
    "\n",
    "    toc = time.time() - tic\n",
    "    print('All saved,took  ' + str(toc) + 'secs')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "\n",
    "# INFO ABOUT YOUR FILES\n",
    "baseFreq = 3.00\n",
    "channels = 2\n",
    "planes = 1\n",
    "\n",
    "# HOW TO FIND COORDINATES CORRESPONDING TO ActualHoloFOV  BOUNDARIES IN YOUR CURRENT MROI SETTING\n",
    "# % find out ActualHoloFOV boundaries using C:\\Users\\MesoDAQ\\Documents\\MATLAB\\MesoSICode\\HScode\\suite2p_pipeline\\convertcoords_HoloFOVtoCurrentFOV.m\n",
    "# % run the following in the MATLAB instance running Scanimage\n",
    "# % the arguments for convertcoords_HoloFOVtoCurrentFOV should already be in the workspace if C:\\Users\\MesoDAQ\\Documents\\MATLAB\\MesoSICode\\makeMasks3D_holeburn.m was run that day\n",
    "# % xynew(:,1) is the horizontal axis, xynew(:,2) is the vertical axis\n",
    "# % set xlb, xub (horizontal axis) and ylb, yub (vertical axis) accordingly in mesocroph5.ipynb \n",
    "# xyorig = [50 50; fullnpix_orig(1)-50 fullnpix_orig(2)-50];\n",
    "# xynew = convertcoords_HoloFOVtoCurrentFOV(hSI,xyorig, fullnpix_orig, fullxsize_orig, fullysize_orig, fullxcenter_orig, fullycenter_orig)\n",
    "# xynew =\n",
    "#          200          67\n",
    "#         1304         617\n",
    "x_start = 200; # horizontal\n",
    "x_end = 1304; # horizontal\n",
    "y_start = 67;\n",
    "y_end = 617;\n",
    "\n",
    "# drive = 'M://'\n",
    "# user_name = 'ICmesoholoexpts_scanimage'\n",
    "drive = 'D://'\n",
    "user_name = 'HS'\n",
    "mouse = 'HS_Chrome2f_3'\n",
    "date = '220922'\n",
    "expList = ['retinotopy0', 'staticICtxi0', 'staticgratings']\n",
    "\n",
    "#where your tifs are\n",
    "tiffFolderList = [drive+ user_name + '//' + mouse +'//' + date + '//' + exp_name + '//' for exp_name in expList]\n",
    "#where you want to save the output hdf5 file\n",
    "suffix = 'ClosedLoop_allch//' #'suite2pAnalysis//'\n",
    "data_path = drive+ user_name + '//' + mouse +'//' + date + '//'\n",
    "out_path = drive+ user_name + '//' + mouse +'//' + date + '//' + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5485dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only took 130 sec for singl\n",
    "x_slice= slice(x_start-1,x_end)\n",
    "y_slice= slice(y_start-1,y_end)\n",
    "tic1 = time.time()\n",
    "\n",
    "os.chdir(data_path)\n",
    "with open('ops.json') as f:\n",
    "    opsjson = json.load(f)\n",
    "print(opsjson.keys())\n",
    "\n",
    "gc.collect()\n",
    "for exp_name, tiff_folder in zip(expList,tiffFolderList):\n",
    "    gc.collect()\n",
    "    pth = Path(tiff_folder)\n",
    "    movs = list(pth.glob('*.tif'))\n",
    "    crop_and_save_h5_meso(movs,tiff_folder,out_path,exp_name, x_slice,y_slice,planes,channels, opsjson)\n",
    "\n",
    "toc1 = time.time()-tic1\n",
    "print(toc1)\n",
    "print('completed h5 conversion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat(out_path+'online_params.mat', {'expList':expList, 'numchannels':channels, 'Nplanes':planes, 'x_start':x_start, 'x_end':x_end, 'y_start':y_start, 'y_end':y_end, 'numtimepointstiffile':numtimepointstiffileagg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa407ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # troubleshooting rewriting tiffs2array_meso\n",
    "# os.chdir(data_path)\n",
    "# with open('ops.json') as f:\n",
    "#     opsjson = json.load(f)\n",
    "# print(opsjson.keys())\n",
    "\n",
    "# tiff_folder = 'D://HS//HS_Ai203_2//220531//retinotopy2//'\n",
    "# pth = Path(tiff_folder)\n",
    "# movie_list = list(pth.glob('*.tif'))\n",
    "# # movs[:10]\n",
    "# data = [LoadTif(str(mov),planes,channels) for mov in movie_list] #CAREFUL HERE should slice after concatenation for Satsuma RF, but NOT for Frankenrig\n",
    "# data = np.concatenate(data)\n",
    "# print('concatenated data from all tiffs, total size: ' + str(data.shape))\n",
    "# print('number of frames: ' + str(data.shape[0]))\n",
    "# print('opsjson[''lines''] shape: ' + str( np.shape(opsjson['lines']) ))\n",
    "# print('opsjson[''dx'']: ' + str(opsjson['dx']))\n",
    "# print(data.shape[0], np.shape(opsjson['lines'])[1], np.shape(opsjson['lines'])[0]*opsjson['dx'][1] )\n",
    "\n",
    "# # # Matlab code for reshaping the data\n",
    "# # # in Matlab, tif data size 608 7620 numframes, and reshaped data size is 1484 3040 numframes\n",
    "# # % reshape the data\n",
    "# # tempdata = zeros(max(Ly), sum(Lx), size(temp,3), 'int16');\n",
    "# # for istrip = 1:numel(Ly)\n",
    "# #     tempdata(1:Ly(istrip), sum(Lx(1:istrip-1))+1:sum(Lx(1:istrip)), :) = permute(temp(:,data.lines{istrip}+1,:),[2 1 3]);\n",
    "# # end\n",
    "# # % crop the data\n",
    "# # temp = tempdata(ylb:yub, xlb:xub, :);\n",
    "# x_slice= slice(x_start,x_end+1)\n",
    "# y_slice= slice(y_start,y_end+1)\n",
    "\n",
    "# tic1 = time.time()\n",
    "# ylen = np.shape(opsjson['lines'])[1]\n",
    "# xlen = np.shape(opsjson['lines'])[0]*opsjson['dx'][1]\n",
    "# xbounds = opsjson['dx'].copy()\n",
    "# xbounds.append(xlen)\n",
    "# datars = np.zeros((data.shape[0], ylen, xlen ))\n",
    "# for istrip in range(np.shape(opsjson['lines'])[0]):\n",
    "#     x_strip= slice(xbounds[istrip],xbounds[istrip+1])\n",
    "#     y_strip= slice(0,ylen)\n",
    "#     print(data[:,opsjson['lines'][istrip],:].shape)\n",
    "#     datars[:,y_strip,x_strip] = data[:,opsjson['lines'][istrip],:].copy()\n",
    "\n",
    "# data = datars[:, y_slice, x_slice].copy()\n",
    "# print('reshaped data size: ' + str(datars.shape))\n",
    "# print('cropped data size: ' + str(data.shape))\n",
    "\n",
    "# toc1 = time.time()-tic1\n",
    "# print(toc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e065c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or if you want to play with the parameters define everything here:\n",
    "# Should comment what everything is, not sure about all the parameters.\n",
    "ops = {'look_one_level_down': 0.0,\n",
    " 'fast_disk': [],\n",
    " 'delete_bin': True,\n",
    " 'mesoscan': False,\n",
    " 'bruker': False,\n",
    " 'h5py': [],\n",
    " 'h5py_key': 'data',\n",
    " 'save_path0': [],\n",
    " 'save_folder': [],\n",
    " 'subfolders': [],\n",
    " 'move_bin': False,\n",
    " 'nplanes': planes,\n",
    " 'nchannels': channels,\n",
    " 'functional_chan': 1,\n",
    " 'tau': 1.5,\n",
    " 'fs': baseFreq/planes,\n",
    " 'force_sktiff': False,\n",
    " 'frames_include': -1,\n",
    " 'multiplane_parallel': False,\n",
    " 'preclassify': 0.0,\n",
    " 'save_mat': True,\n",
    " 'save_NWB': False,\n",
    " 'combined': 1.0,\n",
    " 'aspect': 2.0,\n",
    " 'do_bidiphase': True,\n",
    " 'bidiphase': 0,\n",
    "#  'bidi_corrected': False,\n",
    " 'do_registration': 1,\n",
    " 'two_step_registration': False,\n",
    " 'keep_movie_raw': False, # if two_step_registration = 1 this has to also be true.\n",
    " 'nimg_init': 1000,\n",
    " 'batch_size': 2000,\n",
    " 'maxregshift': 0.1,\n",
    " 'align_by_chan': 1,\n",
    " 'reg_tif': False,\n",
    " 'reg_tif_chan2': False,\n",
    " 'subpixel': 10,\n",
    " 'smooth_sigma_time': 1.0, # default 0\n",
    " 'smooth_sigma': 1.15, # pixels, default 1.15\n",
    " 'th_badframes': 1.0,\n",
    " 'pad_fft': False,\n",
    " 'nonrigid': False,\n",
    " 'block_size': [128, 128],\n",
    " 'snr_thresh': 1.2,\n",
    " 'maxregshiftNR': 5.0,\n",
    " '1Preg': False,\n",
    " 'spatial_hp': 25,\n",
    " 'spatial_hp_reg': 26.0,\n",
    " 'spatial_hp_detect': 25,\n",
    " 'pre_smooth': 0,\n",
    " 'spatial_taper': 50.0,\n",
    " 'roidetect': True,\n",
    " 'spikedetect': True,\n",
    " 'sparse_mode': False,\n",
    " 'diameter': 9,\n",
    " 'spatial_scale': 1, # default 0, if set to 0, then the algorithm determines it automatically (recommend this on the first try)\n",
    " 'connected': True,\n",
    " 'nbinned': 5000, # maximum number of binned frames to use for ROI detection\n",
    " 'max_iterations': 20, # default 20. how many iterations over which to extract cells - at most ops[‘max_iterations’], \n",
    "                        #but usually stops before due to ops[‘threshold_scaling’] criterion\n",
    " 'threshold_scaling': 2.0, # default: 5.0. this controls the threshold at which to detect ROIs.. if you set this higher, then fewer ROIs will be detected\n",
    " 'max_overlap': 0.5, # default: 0.75\n",
    " 'high_pass': 100, #  default: 100. time window size\n",
    " 'inner_neuropil_radius': 2, # default: 2. number of pixels to keep between ROI and neuropil donut\n",
    " 'min_neuropil_pixels': 350, #default: 350. minimum number of pixels used to compute neuropil for each cell\n",
    " 'allow_overlap': False, #default: False. whether or not to extract signals from pixels which belong to two ROIs.\n",
    " 'chan2_thres': 0.65,\n",
    " 'baseline': 'maximin',\n",
    " 'win_baseline': 60.0,\n",
    " 'sig_baseline': 10.0,\n",
    " 'prctile_baseline': 8.0,\n",
    " 'neucoeff': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when running only one plane, takes the same amount of time (12 min in benchmark data)\n",
    "tic2 = time.time()\n",
    "ops['data_path'] = [out_path]\n",
    "ops['h5py'] = out_path\n",
    "ops1 = run_s2p(ops = ops, db = {})\n",
    "toc2 = time.time() - tic2\n",
    "print('Suite2p took ' + str(toc2/60) + 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # suite2p took 681s, or 12 min\n",
    "# #list of files to analyze:\n",
    "# outname = [0]*planes\n",
    "# outdir=[0]*planes \n",
    "\n",
    "# #save output data\n",
    "# for i in range(planes):\n",
    "#     outdir[i]= out_path + 'plane_'+str(i)+'//'\n",
    "#     outname[i]= out_path + 'plane_'+str(i)+'//'\n",
    "\n",
    "# print(outdir)\n",
    "# print(outname[0])\n",
    "\n",
    "# # prepare the processes for each plane\n",
    "# db = []\n",
    "# jobs = []\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     for i in range(planes):\n",
    "#     #while the # of planes is less than the max number of cores we want to use (leaving 2 cores for scanimage)\n",
    "#        if i<os.cpu_count()-2:\n",
    "#     # Define the dataset\n",
    "#             this_db = {\n",
    "#               'h5py':outname[i], # a single h5 file path[p]\n",
    "#               'h5py_key': ['data'],\n",
    "#               'look_one_level_down': True, # whether to look in ALL subfolders when searching for tiffs,\n",
    "#               'save_path0': outdir[i],\n",
    "#               'data_path':  [], # a list of folders with tiffs\n",
    "#                                                      # (or folder of folders with tiffs if look_one_level_down is True, or subfolders is not empty)\n",
    "#               'subfolders': [], # choose subfolders of 'data_path' to look in (optional)\n",
    "#               'fast_disk': [] }# string which specifies where the binary file will be stored (should be an SSD)      \n",
    "#             print(f'Will be processing this data : {this_db}')\n",
    "#             db.append(this_db)\n",
    "#             p = multiprocessing.Process(target=run_s2p, args=(ops,this_db))\n",
    "#             p.start()          \n",
    "#             jobs.append(p)\n",
    "#        else:\n",
    "#             print(f'Hey, do you really want to run all these {i} cores at the same time?')\n",
    "            \n",
    "# #run each plane in parallel\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logging.info('Starting suite2p parallel processing in a different CPU core for each plane')\n",
    "\n",
    "# tic = time.time()\n",
    "# if len(jobs)<os.cpu_count()-2:\n",
    "#     for job in jobs:\n",
    "#         #print ('Parallel processing:  ' + str(outname[i]))\n",
    "#         job.join()\n",
    "#     toc = time.time() - tic\n",
    "#     print('All saved,took  ' + str(toc) + 'secs')\n",
    "# else:\n",
    "#    print(f'Hey, do you really want to run all these {i} cores at the same time?')\n",
    "\n",
    "# toc2 = time.time() - tic1\n",
    "# print('Preprocessing took ' + str(toc2/60) + 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe88e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
