{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0b1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save just the green channel, each MROI strip as an h5 file\n",
    "import os\n",
    "import suite2p\n",
    "from suite2p.run_s2p import run_s2p\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from ScanImageTiffReader import ScanImageTiffReader\n",
    "import h5py\n",
    "from scipy.io import savemat\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "\n",
    "def tiffs2array_nocrop(movie_list, planes, channels):\n",
    "    data = [LoadTif(str(mov),planes,channels) for mov in movie_list] #CAREFUL HERE should slice after concatenation for Satsuma RF, but NOT for Frankenrig\n",
    "    data = np.concatenate(data)\n",
    "    print('concatenated data from all tiffs, total size: ' + str(data.shape))\n",
    "    return data\n",
    "\n",
    "def LoadTif(mov_path,planes,channels):\n",
    "    with ScanImageTiffReader(mov_path) as reader:\n",
    "        data = reader.data()\n",
    "    Tend = int( np.floor(data.shape[0]/(planes*channels)) * (planes*channels) )\n",
    "    t_slice = slice(0,Tend, 1)\n",
    "    data = data[t_slice,:,:]\n",
    "    return data\n",
    "\n",
    "def save_h5_fullmeso(movs,tiff_folder,out_path,expno, planes, channels, opsjson):\n",
    "    # takes a list of tifs. Concatenates each plane, then slices and crops the big movie, then saves it as .h5 file in a folder inside tiff_folder\\\n",
    "    # process multiple folders\n",
    "    # instead of making one h5 per plane, just make it into one big h5\n",
    "    # movs: list of tifs to be processed\n",
    "    # tif folder: directory where the result h5 file will be saved (inside the corresponding plane folder)\n",
    "    # x_slice: cropping of the FOV in x\n",
    "    # x_slice: cropping of the FOV in y\n",
    "    # planes: number of planes for each tif file\n",
    "    # channels: number of channels\"\"\"\n",
    "    tic = time.time()\n",
    "\n",
    "    strips = np.shape(opsjson['lines'])[0]\n",
    "    #exp_name1 = exp_name.replace(\"//\",\"\")\n",
    "    outname = [0]*strips\n",
    "    outdir=[0]*strips\n",
    "    try:\n",
    "        os.mkdir(out_path)\n",
    "    except:\n",
    "        if os.path.isdir(out_path):\n",
    "            print('directory already exists')\n",
    "        else:\n",
    "            print('Cannot create directory.. probably the name is wrong')\n",
    "            print(out_path)\n",
    "            \n",
    "\n",
    "    fullmov = tiffs2array_nocrop(movs,planes,channels)\n",
    "    print(fullmov.shape)\n",
    "    \n",
    "    ylen = np.shape(opsjson['lines'])[1]\n",
    "    xlen = np.shape(opsjson['lines'])[0]*opsjson['dx'][1]\n",
    "    xbounds = opsjson['dx'].copy()\n",
    "    xbounds.append(xlen)\n",
    "    for istrip in range(strips):\n",
    "        stripmov = fullmov[:,opsjson['lines'][istrip],:].copy()\n",
    "        \n",
    "        t_slice = slice(0,stripmov.shape[0], channels)\n",
    "        x_slice = slice(0,stripmov.shape[2], 1)\n",
    "        y_slice = slice(0,stripmov.shape[1], 1)\n",
    "        # x_slice = slice(0,None, 1) #x_slice = slice(0,cropped_mov.shape[2], 1)\n",
    "        # y_slice = slice(0, None, 1)#y_slice = slice(0,cropped_mov.shape[1], 1)\n",
    "        outdir[istrip] = out_path +'plane_'+ str(istrip) + '//'\n",
    "        try:\n",
    "            os.mkdir(outdir[istrip])\n",
    "        except:\n",
    "            print('couldnt make directory')\n",
    "            print(str(outdir[istrip]))\n",
    "\n",
    "        outname[istrip]=outdir[istrip] + 'strip_mov_' + str(expno) + '.h5'\n",
    "        print(f'now processing {outname[istrip]}')\n",
    "        # try:\n",
    "        hf = h5py.File(outname[istrip], 'a')\n",
    "        stripmov1 = stripmov[t_slice, y_slice, x_slice]\n",
    "        print(stripmov1.shape)\n",
    "        hf.create_dataset('data', data=stripmov1, dtype= 'uint16')\n",
    "        hf.close()\n",
    "        print('done saving ' + str(outname[istrip]))\n",
    "        # delete the array and clean RAM\n",
    "        del stripmov1\n",
    "        gc.collect()\n",
    "        # except:\n",
    "        #     print(f'the file {outname[istrip]} already exists')\n",
    "\n",
    "    toc = time.time() - tic\n",
    "    print('All saved,took  ' + str(toc) + 'secs')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5f9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic1 = time.time()\n",
    "\n",
    "# INFO ABOUT YOUR FILES\n",
    "baseFreq = 3.09\n",
    "channels = 1\n",
    "planes = 1\n",
    "\n",
    "# drive = 'M://'\n",
    "# user_name = 'ICmesoholoexpts_scanimage'\n",
    "drive = 'D://'\n",
    "user_name = 'HS'\n",
    "mouse = 'HS_Ai203_2'\n",
    "date = '220531'\n",
    "expList = ['retinotopy2', 'staticICtxi0', 'staticgratiings']\n",
    "#where your tifs are\n",
    "tiffFolderList = [drive+ user_name + '//' + mouse +'//' + date + '//' + exp_name + '//' for exp_name in expList]\n",
    "#where you want to save the output hdf5 file\n",
    "suffix = 'CLFullFOV//' #'suite2pAnalysis//'\n",
    "data_path = drive+ user_name + '//' + mouse +'//' + date + '//'\n",
    "out_path = drive+ user_name + '//' + mouse +'//' + date + '//' + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5485dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fs', 'nplanes', 'nrois', 'mesoscan', 'diameter', 'num_workers_roi', 'keep_movie_raw', 'delete_bin', 'batch_size', 'nimg_init', 'tau', 'combined', 'nonrigid', 'dx', 'dy', 'lines', 'data_path', 'save_path0'])\n",
      "concatenated data from all tiffs, total size: (1096, 7620, 608)\n",
      "(1096, 7620, 608)\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//strip_mov_retinotopy2.h5\n",
      "(1096, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//strip_mov_retinotopy2.h5\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//strip_mov_retinotopy2.h5\n",
      "(1096, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//strip_mov_retinotopy2.h5\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//strip_mov_retinotopy2.h5\n",
      "(1096, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//strip_mov_retinotopy2.h5\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//strip_mov_retinotopy2.h5\n",
      "(1096, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//strip_mov_retinotopy2.h5\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//strip_mov_retinotopy2.h5\n",
      "(1096, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//strip_mov_retinotopy2.h5\n",
      "All saved,took  23.242830753326416secs\n",
      "directory already exists\n",
      "concatenated data from all tiffs, total size: (4677, 7620, 608)\n",
      "(4677, 7620, 608)\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//strip_mov_staticICtxi0.h5\n",
      "(4677, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//strip_mov_staticICtxi0.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//strip_mov_staticICtxi0.h5\n",
      "(4677, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//strip_mov_staticICtxi0.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//strip_mov_staticICtxi0.h5\n",
      "(4677, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//strip_mov_staticICtxi0.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//strip_mov_staticICtxi0.h5\n",
      "(4677, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//strip_mov_staticICtxi0.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//strip_mov_staticICtxi0.h5\n",
      "(4677, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//strip_mov_staticICtxi0.h5\n",
      "All saved,took  114.48857426643372secs\n",
      "directory already exists\n",
      "concatenated data from all tiffs, total size: (740, 7620, 608)\n",
      "(740, 7620, 608)\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//strip_mov_staticgratiings.h5\n",
      "(740, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//strip_mov_staticgratiings.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//strip_mov_staticgratiings.h5\n",
      "(740, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//strip_mov_staticgratiings.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//strip_mov_staticgratiings.h5\n",
      "(740, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//strip_mov_staticgratiings.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//strip_mov_staticgratiings.h5\n",
      "(740, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//strip_mov_staticgratiings.h5\n",
      "couldnt make directory\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//\n",
      "now processing D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//strip_mov_staticgratiings.h5\n",
      "(740, 1484, 608)\n",
      "done saving D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//strip_mov_staticgratiings.h5\n",
      "All saved,took  16.16508388519287secs\n",
      "157.7168745994568\n",
      "completed h5 conversion\n"
     ]
    }
   ],
   "source": [
    "# only took 170 sec\n",
    "tic1 = time.time()\n",
    "\n",
    "os.chdir(data_path)\n",
    "with open('ops.json') as f:\n",
    "    opsjson = json.load(f)\n",
    "print(opsjson.keys())\n",
    "\n",
    "gc.collect()\n",
    "for exp_name, tiff_folder in zip(expList,tiffFolderList):\n",
    "    gc.collect()\n",
    "    pth = Path(tiff_folder)\n",
    "    movs = list(pth.glob('*.tif'))\n",
    "    save_h5_fullmeso(movs,tiff_folder,out_path,exp_name, planes, channels, opsjson)\n",
    "\n",
    "toc1 = time.time()-tic1\n",
    "print(toc1)\n",
    "print('completed h5 conversion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bdac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two settings that are most critical for speed in low SNR recordings:\n",
    "# 'max_iterations' and 'threshold_scaling'\n",
    "ops = {'look_one_level_down': 0.0,\n",
    " 'fast_disk': [],\n",
    " 'delete_bin': True,\n",
    " 'mesoscan': False,\n",
    " 'bruker': False,\n",
    " 'h5py': [],\n",
    " 'h5py_key': 'data',\n",
    " 'save_path0': [],\n",
    " 'save_folder': [],\n",
    " 'subfolders': [],\n",
    " 'move_bin': False,\n",
    " 'nplanes': 1,\n",
    " 'nchannels': 1,\n",
    " 'functional_chan': 1,\n",
    " 'tau': 1.5,\n",
    " 'fs': baseFreq/planes,\n",
    " 'force_sktiff': False,\n",
    " 'frames_include': -1,\n",
    " 'multiplane_parallel': False,\n",
    " 'preclassify': 0.0,\n",
    " 'save_mat': True,\n",
    " 'save_NWB': False,\n",
    " 'combined': 1.0,\n",
    " 'aspect': 2.0,\n",
    " 'do_bidiphase': True,\n",
    " 'bidiphase': 0,\n",
    "#  'bidi_corrected': False,\n",
    " 'do_registration': 1,\n",
    " 'two_step_registration': False,\n",
    " 'keep_movie_raw': False, # if two_step_registration = 1 this has to also be true.\n",
    " 'nimg_init': 1000,\n",
    " 'batch_size': 2000,\n",
    " 'maxregshift': 0.1,\n",
    " 'align_by_chan': 1,\n",
    " 'reg_tif': False,\n",
    " 'reg_tif_chan2': False,\n",
    " 'subpixel': 10,\n",
    " 'smooth_sigma_time': 1.0, # default 0\n",
    " 'smooth_sigma': 1.15, # pixels, default 1.15\n",
    " 'th_badframes': 1.0,\n",
    " 'pad_fft': False,\n",
    " 'nonrigid': False,\n",
    " 'block_size': [128, 128],\n",
    " 'snr_thresh': 1.2,\n",
    " 'maxregshiftNR': 5.0,\n",
    " '1Preg': False,\n",
    " 'spatial_hp': 25,\n",
    " 'spatial_hp_reg': 26.0,\n",
    " 'spatial_hp_detect': 25,\n",
    " 'pre_smooth': 0,\n",
    " 'spatial_taper': 50.0,\n",
    " 'roidetect': True,\n",
    " 'spikedetect': True,\n",
    " 'sparse_mode': False,\n",
    " 'diameter': 9,\n",
    " 'spatial_scale': 1, # default 0, if set to 0, then the algorithm determines it automatically (recommend this on the first try)\n",
    " 'connected': True,\n",
    " 'nbinned': 5000, # maximum number of binned frames to use for ROI detection\n",
    " 'max_iterations': 20, # default 20. how many iterations over which to extract cells - at most ops[‘max_iterations’], \n",
    "                        #but usually stops before due to ops[‘threshold_scaling’] criterion\n",
    " 'threshold_scaling': 2.0, # default: 5.0. this controls the threshold at which to detect ROIs.. if you set this higher, then fewer ROIs will be detected\n",
    " 'max_overlap': 0.5, # default: 0.75\n",
    " 'high_pass': 100, #  default: 100. time window size\n",
    " 'inner_neuropil_radius': 2, # default: 2. number of pixels to keep between ROI and neuropil donut\n",
    " 'min_neuropil_pixels': 350, #default: 350. minimum number of pixels used to compute neuropil for each cell\n",
    " 'allow_overlap': False, #default: False. whether or not to extract signals from pixels which belong to two ROIs.\n",
    " 'chan2_thres': 0.65,\n",
    " 'baseline': 'maximin',\n",
    " 'win_baseline': 60.0,\n",
    " 'sig_baseline': 10.0,\n",
    " 'prctile_baseline': 8.0,\n",
    " 'neucoeff': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5f827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//', 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//', 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//', 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//', 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//']\n",
      "D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//\n"
     ]
    }
   ],
   "source": [
    "#list of files to analyze:\n",
    "strips = np.shape(opsjson['lines'])[0]\n",
    "outname = [0]*strips\n",
    "outdir=[0]*strips \n",
    "\n",
    "#save output data\n",
    "for i in range(strips):\n",
    "    outdir[i]= out_path + 'plane_'+str(i)+'//'\n",
    "    outname[i]= out_path + 'plane_'+str(i)+'//'\n",
    "\n",
    "print(outdir)\n",
    "print(outname[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0c0e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be processing this data : {'h5py': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//', 'h5py_key': ['data'], 'look_one_level_down': True, 'save_path0': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_0//', 'data_path': [], 'subfolders': [], 'fast_disk': []}\n",
      "Will be processing this data : {'h5py': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//', 'h5py_key': ['data'], 'look_one_level_down': True, 'save_path0': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_1//', 'data_path': [], 'subfolders': [], 'fast_disk': []}\n",
      "Will be processing this data : {'h5py': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//', 'h5py_key': ['data'], 'look_one_level_down': True, 'save_path0': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_2//', 'data_path': [], 'subfolders': [], 'fast_disk': []}\n",
      "Will be processing this data : {'h5py': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//', 'h5py_key': ['data'], 'look_one_level_down': True, 'save_path0': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_3//', 'data_path': [], 'subfolders': [], 'fast_disk': []}\n",
      "Will be processing this data : {'h5py': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//', 'h5py_key': ['data'], 'look_one_level_down': True, 'save_path0': 'D://HS//HS_Ai203_2//220531//CLFullFOV//plane_4//', 'data_path': [], 'subfolders': [], 'fast_disk': []}\n"
     ]
    }
   ],
   "source": [
    "# prepare the processes for each plane\n",
    "db = []\n",
    "jobs = []\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    for i in range(strips):\n",
    "    #while the # of planes is less than the max number of cores we want to use (leaving 2 cores for scanimage)\n",
    "       if i<os.cpu_count()-2:\n",
    "    # Define the dataset\n",
    "            this_db = {\n",
    "              'h5py':outname[i], # a single h5 file path[p]\n",
    "              'h5py_key': ['data'],\n",
    "              'look_one_level_down': True, # whether to look in ALL subfolders when searching for tiffs,\n",
    "              'save_path0': outdir[i],\n",
    "              'data_path':  [], # a list of folders with tiffs\n",
    "                                                     # (or folder of folders with tiffs if look_one_level_down is True, or subfolders is not empty)\n",
    "              'subfolders': [], # choose subfolders of 'data_path' to look in (optional)\n",
    "              'fast_disk': [] }# string which specifies where the binary file will be stored (should be an SSD)      \n",
    "            print(f'Will be processing this data : {this_db}')\n",
    "            db.append(this_db)\n",
    "            p = multiprocessing.Process(target=run_s2p, args=(ops,this_db))\n",
    "            p.start()          \n",
    "            jobs.append(p)\n",
    "       else:\n",
    "            print(f'Hey, do you really want to run all these {i} cores at the same time?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b34428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting suite2p parallel processing in a different CPU core for each MROI strips\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-904d5502dc4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#print ('Parallel processing:  ' + str(outname[i]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'All saved,took  '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'secs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tic' is not defined"
     ]
    }
   ],
   "source": [
    "# suite2p took 2244.20 sec, or 38 min\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info('Starting suite2p parallel processing in a different CPU core for each MROI strips')\n",
    "\n",
    "#run each strips in parallel\n",
    "tic = time.time()\n",
    "if len(jobs)<os.cpu_count()-2:\n",
    "    for job in jobs:\n",
    "        #print ('Parallel processing:  ' + str(outname[i]))\n",
    "        job.join()\n",
    "    toc = time.time() - tic\n",
    "    print('All saved,took  ' + str(toc) + 'secs')\n",
    "else:\n",
    "   print(f'Hey, do you really want to run all these {i} cores at the same time?')\n",
    "\n",
    "toc2 = time.time() - tic1\n",
    "print('Preprocessing took ' + str(toc2/60) + 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2460f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
